{"cells":[{"cell_type":"markdown","source":["#Fine-tune implemented on LLaMA2's \"meta-llama/Llama-2-7b-chat-hf\" Language Model with Logs Graph through TensorBoard using  <big>  Dictionary Type</big> conversational dataset of Harry Potter Character chosen from the Harry Potter and the Philosopher's Stone (film)\n","\n","##<b>Available @ https://huggingface.co/Komala/hpv2_finetuned-llama-7b-chat-hf\n","Model Name: Komala/hpv2_finetuned-llama-7b-chat-hf<br/>\n","Base Model: \"meta-llama/Llama-2-7b-chat-hf\"</b><br/>\n","NOTE: A GPU is needed for quantization. Llama-2-7b-chat-hf fine-tuning is possible only through Quantization and only with GPU(or T4GPU on Colab) and not possible to fine-tune with CPU.\n","<big><b>Fine-tuning of this \"meta-llama/Llama-2-7b-chat-hf\" big model made possible by applying 4-bit Quantization via LoRA METHOD.</b></big>\n","### Trained with epochs=3,  per_device_train_batch_size=4, optim=\"paged_adamw_32bit\", mainly logged every 10 steps through logging_steps=10 setting resulted in extensive fine-tuning time of more than 24 hours despite fine-tuning on a GPU with 32 GB RAM provisioned laptop through Anaconda interface.\n","Conversational (Dialog) Dataset formed utilizing the following Open Data Resources<br/>\n","<b>1. Story outline from Wikipedia.org</b>\n"," https://en.wikipedia.org/wiki/Harry_Potter_and_the_Philosopher%27s_Stone_(film)<br/>\n","<b>2. Transript from warnerbros.fandom.com Website</b>\n","https://warnerbros.fandom.com/wiki/Harry_Potter_and_the_Philosopher%27s_Stone/Transcript\n","<br/>\n","<b>3. Full Story and scenes screenplay from J. K. Rowling's original Storybook from free online Book</b>\n","https://kalyankrishna4886.wordpress.com/wp-content/uploads/2013/09/harry-potter-book-collection-1-4.pdf<br/>\n","\n","\n","Concept & Code Reference: https://medium.com/@kevaldekivadiya2415/fine-tuning-llama-2-your-path-to-chemistry-text-perfection-aa4c54ff5790"],"metadata":{"id":"KGXRPyDf6oFo"}},{"cell_type":"markdown","source":["# Installed the required packages:\n","<div align='justify'>\n","<b>Installed the upgraded version of Hugging Face platforms' \"transformers\" library along with the \"accelerate\" library, both for use with PyTorch. Also, Installed  \"huggingface-hub\" and \"datasets\".</b><br/>\n","\n","1) <b>transformers</b> library provides state-of-the-art pre-trained models for natural language processing (NLP) tasks, such as text classification, language modeling, translation, and more. It also includes utilities for fine-tuning and customizing these models for specific tasks.\n","<br/>\n","2) <b> datasets</b> package streamlines the process of accessing, preprocessing, and using datasets in conjunction with LLMs, contributing to a more efficient and standardized workflow in natural language processing tasks. The package includes functionality for preprocessing datasets, such as tokenization, batching, and data augmentation. These preprocessing steps are crucial for preparing the data to be fed into LLMs during training.\n","\n","3) <b>PEFT(Parameter-Efficient Fine-Tuning)</b> methods enable efficient adaptation of large pretrained models to various downstream applications by only fine-tuning a small number of (extra) model parameters instead of all the model's parameters. This significantly decreases the computational and storage costs.\n","\n","4) <b>accelerate</b> is a library developed by Hugging Face. It provides utilities for efficient distributed training of PyTorch models. It is installed mainly to perform training of large models on multiple GPUs or across multiple machines.\n","Thus, installing both transformers and accelerate are especially useful for training large NLP models efficiently. <br/>\n","\n","5) <b>bitsandbytes: </b>A package designed for low-level manipulation of binary data, providing tools and utilities for working with individual bits and bytes.\n","\n","6) <b>trl:</b> The Text Representation Learning (TRL) package is used for training and evaluating models for natural language understanding tasks, focusing on learning effective representations of text data.\n","\n","7) <b>safetensors:</b> A package developed to ensure the safe and secure handling of tensors in machine learning and deep learning applications, addressing issues such as numerical stability, overflow, and underflow.\n","\n","8) <b>huggingface-hub</b> library provides the Hugging Face Hub for accessing their platform, allowing users to push fine-tuned models under their login using privileged access, by providing their login's write TokenID for pushing the fine-tuned model<br/>\n","9)<b>tensorboard</b>: TensorBoard is a visualization toolkit for machine learning experimentation. It helps users track and visualize various metrics such as loss, accuracy, and gradients during training, inspect model architectures, and analyze performance using interactive visualizations like histograms, line plots, and embeddings\n","  </div>"],"metadata":{"id":"EAdtvgGJ-DZE"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95422,"status":"ok","timestamp":1714212267570,"user":{"displayName":"Komala T","userId":"16953111298723154484"},"user_tz":-60},"id":"p0-94evj3sDe","outputId":"45de419b-8f0a-49d0-c822-9c6d97ea9534"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Collecting datasets\n","  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft\n","  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate\n","  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bitsandbytes\n","  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trl\n","  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m143.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n","  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Collecting tyro>=0.5.11 (from trl)\n","  Downloading tyro-0.8.3-py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m145.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m213.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m219.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m172.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m179.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m242.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m200.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n","Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n","  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Installing collected packages: xxhash, shtab, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, tyro, nvidia-cusolver-cu12, datasets, bitsandbytes, accelerate, trl, peft\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.20.3\n","    Uninstalling huggingface-hub-0.20.3:\n","      Successfully uninstalled huggingface-hub-0.20.3\n","Successfully installed accelerate-0.29.3 bitsandbytes-0.43.1 datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 shtab-1.7.1 trl-0.8.6 tyro-0.8.3 xxhash-3.4.1\n"]}],"source":["!pip install transformers datasets peft accelerate bitsandbytes trl safetensors torch --no-cache"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30246,"status":"ok","timestamp":1714212297802,"user":{"displayName":"Komala T","userId":"16953111298723154484"},"user_tz":-60},"id":"kNasG0bn5q0k","outputId":"8446f889-12ab-4e49-f2d5-6e44b5d00da2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.0)\n","Collecting transformers[torch]\n","  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.40.0\n","    Uninstalling transformers-4.40.0:\n","      Successfully uninstalled transformers-4.40.0\n","Successfully installed transformers-4.40.1\n"]}],"source":["!pip install transformers[torch] accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15458,"status":"ok","timestamp":1710672407290,"user":{"displayName":"Komala STS","userId":"02932540021665926065"},"user_tz":0},"id":"XFFl976y5yIn","outputId":"d9a8c4f6-bf52-4346-f1d9-7ee2d823ae40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface-hub in c:\\users\\komala\\anaconda3\\lib\\site-packages (0.20.3)\n","Requirement already satisfied: filelock in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (3.9.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (2023.10.0)\n","Requirement already satisfied: requests in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.9.0)\n","Requirement already satisfied: packaging>=20.9 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from huggingface-hub) (23.2)\n","Requirement already satisfied: colorama in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (2023.11.17)\n"]}],"source":["!pip install huggingface-hub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13122,"status":"ok","timestamp":1710672502898,"user":{"displayName":"Komala STS","userId":"02932540021665926065"},"user_tz":0},"id":"a7idpb5i5gCn","outputId":"58f19b22-ec41-49a4-c565-be1260820781"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: bitsandbytes in c:\\users\\komala\\anaconda3\\lib\\site-packages (0.43.0)\n","Requirement already satisfied: torch in c:\\users\\komala\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.2.0+cu121)\n","Requirement already satisfied: numpy in c:\\users\\komala\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.24.3)\n","Requirement already satisfied: filelock in c:\\users\\komala\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.9.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (4.9.0)\n","Requirement already satisfied: sympy in c:\\users\\komala\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (1.11.1)\n","Requirement already satisfied: networkx in c:\\users\\komala\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.1.2)\n","Requirement already satisfied: fsspec in c:\\users\\komala\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (2023.10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"]}],"source":["# !pip install accelerate\n","!pip install -i https://pypi.org/simple/ bitsandbytes\n","# ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the\n","#latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\n","# NOTE: If your import is failing due to a missing package, you can manually install dependencies using either !pip or !apt.\n"]},{"cell_type":"markdown","source":["#STEP1: Loaded the cleaned conversational dataset from CSV file to form a Dictionary Type Hugging Face Dataset\n"],"metadata":{"id":"fqYYDOn6COeb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4984,"status":"ok","timestamp":1710672511580,"user":{"displayName":"Komala STS","userId":"02932540021665926065"},"user_tz":0},"id":"uxJv6QOa8U3E","outputId":"312bd7e0-abe4-4055-fc71-108d70ce507f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Size: 865\n","{'instruction': \"Harry Potter BOT from the Movie Harry Potter and the Philospher's Stone\", 'input': 'How did you feel when you first laid eyes on the Hogwarts Express at Platform 9¾?', 'output': 'It was like stepping into a whole new world! The sight of the train, bustling with students and magical creatures, filled me with excitement and wonder.'}\n"]}],"source":["import pandas as pd\n","from datasets import Dataset\n","from random import randrange\n","\n","# Loading dataset from CSV file. it contains three columns namely: instruction, input and output\n","#formated using the jupyter notebook ChangeDataformatForLlama2_HPCSVw9.jpynb\n","df = pd.read_csv(\"HP_dialogdataset_llama2.csv\")\n","\n","# Convert DataFrame to Data Dictionary to match the Hugging Face Dataset format\n","dataset_dict = {\n","    \"instruction\": df[\"instruction\"].tolist(),\n","    \"input\": df[\"input\"].tolist(),\n","    \"output\": df[\"output\"].tolist(),\n","}\n","\n","# Creating Dataset object\n","dataset = Dataset.from_dict(dataset_dict)\n","\n","print(f\"Dataset Size: {len(dataset)}\")\n","print(dataset[randrange(len(dataset))])"]},{"cell_type":"markdown","metadata":{"id":"6SgnbB__D0mP"},"source":["## LLAMA2's Specific Data Template adopted to form the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrmVpOqY3yv6"},"outputs":[],"source":["def format_prompt(sample):\n","    return f\"\"\"\n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{sample[\"instruction\"]}\n","\n","### Input:\n","{sample[\"input\"]}\n","\n","### Response:\n","{sample[\"output\"]}\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":224,"status":"ok","timestamp":1710672512789,"user":{"displayName":"Komala STS","userId":"02932540021665926065"},"user_tz":0},"id":"Ci3OAwWT3yyM","outputId":"12d8b2d6-8bbb-474b-9796-f3d5794b24c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Harry Potter BOT from the Movie Harry Potter and the Philospher's Stone\n","\n","### Input:\n","Harry, why was Hermione disappointed that you didn't find out who Nicolas Flamel was during the holidays?\n","\n","### Response:\n","Hermione believed that discovering Nicolas Flamel's identity could provide valuable insight into the mysteries surrounding the Sorcerer's Stone and its connection to recent events at Hogwarts.\n","\n"]}],"source":["from random import randrange\n","\n","print(format_prompt(dataset[randrange(len(dataset))]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331,"referenced_widgets":["bfaca87bb70a4a68a23f40bab72b0918","c8ab185ae2b24f3da405da915c732962","2867647f18d64e6899b8131ece7bdd45","ada7edec2c704839bdb4f9a404643208","540d40f0757f4175a36e5a93956b15a1","cd271fca156b40fd9aea9f3e6a887fa1","7182ca06568f45e1807d9588e47474fd","502475916df248bbab233bb564bce019","6a559a7b21454b8fa12f54ec556b0c85","25c7b221f5ef4718adb0a9ae5689e9e4","7eef7070f3024a2f8d628710d69951e2","d2d0a578ea2f42dbadbe2d242ccc4a83","a5bd8bbcc33e401d84b2d199ea6cfab5","b5c2c58f59b04718b5c3878edab1c82a","190d61c10c6846d8bbcbd97eccf035eb","17eb939dca434ceabe22cee05d01e3c2","dbc8160942614e6ca9ca264074f185c4"]},"executionInfo":{"elapsed":734,"status":"ok","timestamp":1714222211854,"user":{"displayName":"Komala T","userId":"16953111298723154484"},"user_tz":-60},"id":"h0PYAzb75lin","outputId":"2a67c849-56de-4376-9935-0f9de1936d7c"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfaca87bb70a4a68a23f40bab72b0918"}},"metadata":{}}],"source":["#importing notebook_login from huggingface_hub for logging into Hugging Face Hub before pushing the Trained Model\n","from huggingface_hub import notebook_login\n","notebook_login()\n","#  Must give Write TokenID"]},{"cell_type":"markdown","metadata":{"id":"55qd2deeD0mR"},"source":["# Fine-tuning the base model \"Llama 2\" using trl and the SFTTrainer"]},{"cell_type":"markdown","metadata":{"id":"ytCLoSV7D0mS"},"source":["#  Instantiated the base model(meta-llama/Llama-2-7b-chat-hf) using 4-bit quantization using BitsAndBytesConfig() method. And Instantiated the base model's tokenizer\n","The packages bitsandbytes, 4-bit quantization and LoRA are used here to make the Llama2 LLM  even more accessible for fine-tuning despite of its large size due to more parameters. Below code does the Quantization of the larg  LLAMA2  pre-trained model to 4 bits and freeze it. This will attach small, trainable adapter layers (named as LoRA). Finally, fine-tuning only the adapter layers while utilizing the frozen quantized model for context."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514,"referenced_widgets":["42eb7d11aafb48aca9169f89bce1c0f5"]},"executionInfo":{"elapsed":2530,"status":"error","timestamp":1710673731744,"user":{"displayName":"Komala STS","userId":"02932540021665926065"},"user_tz":0},"id":"3fJI6m1z4yUe","outputId":"1b3d44b3-12a4-46a6-f18b-e847237af91a"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42eb7d11aafb48aca9169f89bce1c0f5","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#Fine-tuning the base model \"Llama 2\" using trl and the SFTTrainer\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# Hugging Face base model name\n","model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","use_flash_attention = False\n","\n","# BitsAndBytesConfig int-4 configuration done to perform quantization method to reduce the memory size of the large LLAMA2 model.\n","#So as make it accessible for finetuning\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n",")\n","\n","# Loading the Base model with quantization configuration\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    use_cache=False,\n","    #use_flash_attention_2=use_flash_attention,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16\n",")\n","\n","model.config.pretraining_tp = 1\n","\n","# Loading the Base Model's tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{"id":"sX-ikhqvD0mT"},"source":["# INSTANTIATING THE LoRA CONFIGURATION BASED ON THE QLoRA PAPER\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idTyAlnO3y1P"},"outputs":[],"source":["#INSTANTIATING THE LoRA CONFIGURATION BASED ON THE QLoRA PAPER “QLoRA: Efficient Finetuning of Quantized LLMs”\n","from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n","\n","# LoRA config based on QLoRA paper\n","peft_config = LoraConfig(\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    r=16,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","# Prepare model for training\n","model = prepare_model_for_kbit_training(model)\n","model = get_peft_model(model, peft_config)"]},{"cell_type":"markdown","metadata":{"id":"8w0FTfYW57QB"},"source":["# Setting the hyperparameters as the trainer arguments for finetuning the LLAMA2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sReWzOd5TVj"},"outputs":[],"source":["# Setting the hyperparameters as the trainer arguments for finetuning the LLAMA2\n","from transformers import TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir=\"hpv2_finetuned-llama-7b-chat-hf\",\n","    logging_dir=\"logs\",  # Specify the directory where logs will be saved for monitoring and analysis\n","    num_train_epochs=3, # number of epochs (training iterations) for fine-tuning\n","    per_device_train_batch_size=4,  #batch size per GPU during training\n","    gradient_accumulation_steps=2,  #This Controls the number of gradient accumulation steps before performing parameter updates for handling large batch sizes.\n","    gradient_checkpointing=True, #a memory-saving technique during training.\n","    optim=\"paged_adamw_32bit\",  #This define optimizer used for training, in this case, \"paged_adamw_32bit\".\n","    logging_steps=10,  #This specifies the frequency of logging training metrics during each epoch\n","    save_strategy=\"epoch\", #the strategy for saving model checkpoints, in this case, \"epoch\".\n","    learning_rate=2e-4, #Sets the initial learning rate for the optimizer.\n","    fp16=True,  #16-bit floating-point precision to accelerate training and reduce memory usage\n","    max_grad_norm=0.3,  #Sets the maximum gradient norm for gradient clipping during training\n","    warmup_ratio=0.03,  #Specifies the ratio of warm-up steps to total training steps, used for gradual learning rate warm-up\n","    lr_scheduler_type=\"constant\",  # learning rate scheduler used during training set as \"constant\".\n","    disable_tqdm=False,  # Not to disable the tqdm progress bar during training\n","    push_to_hub = True, #pushing the fine-tuned model to HuggingFace model hub\n","    report_to=\"tensorboard\" # Enable Logs reporting to TensorBoard\n",")"]},{"cell_type":"markdown","metadata":{"id":"VQs8TaXKD0mU"},"source":["# \"SFTTrainer\" was used for training the model instead of \"Trainer\".\n","SFTTrainer could be used when we have a pre-trained model and a relatively smaller dataset, and want a simpler and faster fine-tuning experience with efficient memory usage. I could be noted that the SFTTrainer offers seamless integration with peft, simplifying the process of efficiently fine-tuning LLMs with instructions. Hence, in the below code the LoRa congiruration is created as \"LoRAConfig\" and supplyed it to the trainer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSPNC9Ex5TY0","colab":{"referenced_widgets":["b89b2afc4e6d442aa0098825af96765c"]},"outputId":"c6f7bd6b-9fb0-4474-da50-59a34581f68f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b89b2afc4e6d442aa0098825af96765c","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Komala\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["#Setting the Trainer for LLAMA2 model using the \"SFTTrainer\"\n","from trl import SFTTrainer\n","\n","max_seq_length = 1024 # max sequence length for model and packing of the dataset\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    formatting_func=format_prompt,\n","    args=args,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uw4nXnvg5Tbn","scrolled":false,"outputId":"96e2ae09-8115-4ff4-c3c7-0b0fcbf3362b"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\Komala\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","C:\\Users\\Komala\\anaconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [48/48 21:47:11, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>1.137200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.859700</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.779400</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.701800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["C:\\Users\\Komala\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","C:\\Users\\Komala\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]}],"source":["# Training the LLAMA2 model using the \"SFTTrainer\" trainer on the Harry potter dialog dataset and saving the model in local disk\n","trainer.train()\n","\n","# Saving the trained model in local disk\n","trainer.save_model()"]},{"cell_type":"markdown","source":["### Pushing the completly trained/ finetuned LLAMA2 model to the Hugging Face hub"],"metadata":{"id":"jhhrUKu0UlJc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NdpIIqFn3y4J","outputId":"90b6d855-1213-4280-8d6a-bbe16e6b7c8c"},"outputs":[{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Komala/hpv2_finetuned-llama-7b-chat-hf/commit/0c1e48ccdd16ec603a54f51429154190616dbccd', commit_message='End of training', commit_description='', oid='0c1e48ccdd16ec603a54f51429154190616dbccd', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Pushing the completly trained/ finetuned LLAMA2 model to the Hugging Face hub\n","trainer.push_to_hub()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fViNns_ID0mW","outputId":"dc9a194d-777e-4466-a9dd-a095d5c0daaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorboard in c:\\users\\komala\\anaconda3\\lib\\site-packages (2.16.2)\n","Requirement already satisfied: absl-py>=0.4 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (1.62.0)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (3.4.1)\n","Requirement already satisfied: numpy>=1.12.0 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (1.24.3)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (4.25.2)\n","Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (68.0.0)\n","Requirement already satisfied: six>1.9 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from tensorboard) (2.2.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\komala\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n"]}],"source":["!pip install tensorboard\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"02YlTDMiD0mX","outputId":"c3c3e6eb-4e1f-47cc-d2af-f8aaae0ed0a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-822e8f36c031199\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-822e8f36c031199\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6008;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# Loading TensorBoard to view the training logs of LLAMA2 model instances including metrics such as epoch, grad_norm, learning_rate, train_runtime, and train_samples_per_second.\n","%load_ext tensorboard\n","%tensorboard --logdir C:/acodes/LLAMA2/logs\n","%reload_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Vvl1XF6CD0mY","outputId":"c2075867-0f51-431f-925b-410a0441dac3"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]},{"data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 17600), started 2 days, 4:02:01 ago. (Use '!kill 17600' to kill it.)"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-1a3d1fa7bc8960a9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-1a3d1fa7bc8960a9\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# %load_ext tensorboard\n","# %tensorboard --logdir results/runs\n","# #report_to=\"tensorboard\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0ntjF0bD0ma","outputId":"910e8a35-4eaa-4099-9bad-e8244017ffd6"},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\acodes\\LLAMA2\\logs\n"]},{"data":{"text/plain":["ERROR: Failed to launch TensorBoard (exited with 2).\n","Contents of stderr:\n","TensorFlow installation not found - running with reduced feature set.\n","usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n","                   [--host ADDR] [--bind_all] [--port PORT]\n","                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n","                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n","                   [--grpc_creds_type {local,ssl,ssl_dev}]\n","                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n","                   [--db URI] [--db_import] [--inspect] [--version_tb]\n","                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n","                   [--window_title TEXT] [--max_reload_threads COUNT]\n","                   [--reload_interval SECONDS] [--reload_task TYPE]\n","                   [--reload_multifile BOOL]\n","                   [--reload_multifile_inactive_secs SECONDS]\n","                   [--generic_data TYPE]\n","                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n","                   [--detect_file_replacement BOOL]\n","                   {serve} ...\n","tensorboard: error: argument --logdir: expected one argument"]},"metadata":{},"output_type":"display_data"}],"source":["# #tensorboard --logdir 'http://localhost:8888/notebooks/acodes/LLAMA2/logs'\n","# # tensorboard --logdir C:\\acodes\\LLAMA2\\logs\n","\n","# %cd C:/acodes/LLAMA2/logs\n","# %tensorboard --logdir\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bfaca87bb70a4a68a23f40bab72b0918":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_c8ab185ae2b24f3da405da915c732962","IPY_MODEL_2867647f18d64e6899b8131ece7bdd45","IPY_MODEL_ada7edec2c704839bdb4f9a404643208","IPY_MODEL_540d40f0757f4175a36e5a93956b15a1","IPY_MODEL_cd271fca156b40fd9aea9f3e6a887fa1"],"layout":"IPY_MODEL_7182ca06568f45e1807d9588e47474fd"}},"c8ab185ae2b24f3da405da915c732962":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_502475916df248bbab233bb564bce019","placeholder":"​","style":"IPY_MODEL_6a559a7b21454b8fa12f54ec556b0c85","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"2867647f18d64e6899b8131ece7bdd45":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_25c7b221f5ef4718adb0a9ae5689e9e4","placeholder":"​","style":"IPY_MODEL_7eef7070f3024a2f8d628710d69951e2","value":""}},"ada7edec2c704839bdb4f9a404643208":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_d2d0a578ea2f42dbadbe2d242ccc4a83","style":"IPY_MODEL_a5bd8bbcc33e401d84b2d199ea6cfab5","value":true}},"540d40f0757f4175a36e5a93956b15a1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_b5c2c58f59b04718b5c3878edab1c82a","style":"IPY_MODEL_190d61c10c6846d8bbcbd97eccf035eb","tooltip":""}},"cd271fca156b40fd9aea9f3e6a887fa1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17eb939dca434ceabe22cee05d01e3c2","placeholder":"​","style":"IPY_MODEL_dbc8160942614e6ca9ca264074f185c4","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"7182ca06568f45e1807d9588e47474fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"502475916df248bbab233bb564bce019":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a559a7b21454b8fa12f54ec556b0c85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25c7b221f5ef4718adb0a9ae5689e9e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eef7070f3024a2f8d628710d69951e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2d0a578ea2f42dbadbe2d242ccc4a83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5bd8bbcc33e401d84b2d199ea6cfab5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5c2c58f59b04718b5c3878edab1c82a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"190d61c10c6846d8bbcbd97eccf035eb":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"17eb939dca434ceabe22cee05d01e3c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbc8160942614e6ca9ca264074f185c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}